# 1、引言

对于预测性的系统来说，特征工程起到了至关重要的作用。特征工程中，挖掘交叉特征是至关重要的。交叉特征指的是两个或多个原始特征之间的交叉组合。例如，在新闻推荐场景中，**一个三阶交叉特征为`AND(user_organization=msra,item_category=deeplearning,time=monday_morning)`**,它表示当前用户的工作单位为微软亚洲研究院，当前文章的类别是与深度学习相关的，并且推送时间是周一上午。

传统的推荐系统中，挖掘交叉特征主要依靠人工提取，这种做法主要有以下三种缺点：

- 1）**重要的特征都是与应用场景息息相关的，针对每一种应用场景，工程师们都需要首先花费大量时间和精力深入了解数据的规律之后才能设计、提取出高效的高阶交叉特征，因此人力成本高昂**；
- 2）**原始数据中往往包含大量稀疏的特征，例如用户和物品的ID，交叉特征的维度空间是原始特征维度的乘积，因此很容易带来维度灾难的问题**；
- 3）**人工提取的交叉特征无法泛化到未曾在训练样本中出现过的模式中**。

因此自动学习特征间的交互关系是十分有意义的。**目前大部分相关的研究工作是基于因子分解机的框架，利用多层全连接神经网络去自动学习特征间的高阶交互关系，例如FNN、PNN和DeepFM等。其缺点是模型学习出的是隐式的交互特征，其形式是未知的、不可控的**；**<font color= blue>同时它们的特征交互是发生在元素级（bit-wise）而不是特征向量之间（vector-wise），这一点违背了因子分解机的初衷</font>**。

来自Google的团队在KDD 2017 AdKDD&TargetAD研讨会上提出了**DCN模型，旨在显式（explicitly）地学习高阶特征交互，其优点是模型非常轻巧高效，但缺点是最终模型的表现形式是一种很特殊的向量扩张，同时特征交互依旧是发生在元素级上。**

我们用下图来回顾一下DCN的实现：

![img](img/1.png)

下面是我对文中提到的两个重要概念的理解：

## bit-wise VS vector-wise

假设隐向量的维度为3维:
- 如果两个特征(对应的向量分别为(a1,b1,c1)和(a2,b2,c2)的话）在进行交互时，交互的形式类似于f(w1 * a1 * a2,w2 * b1 * b2 ,w3 * c1 * c2)的话，此时我们认为特征交互是发生在**元素级（bit-wise）**上。
- 如果特征交互形式类似于 f(w * (a1 * a2 ,b1 * b2,c1 * c2))的话，那么我们认为**特征交互是发生在特征向量级（vector-wise）**。

## explicitly VS implicitly

**显式的特征交互和隐式的特征交互。**以两个特征为例xi和xj，在经过一系列变换后，我们可以表示成 wij * (xi * xj)的形式，就可以认为是显式特征交互，否则的话，是隐式的特征交互。

微软亚洲研究院社会计算组提出了一种**极深因子分解机模型（xDeepFM），不仅能同时以显式和隐式的方式自动学习高阶的特征交互，使特征交互发生在向量级，还兼具记忆与泛化的学习能力。**

我们接下来就来看看**xDeepFM这个模型**是怎么做的吧！

# 2、xDeepFM模型介绍

## 2.1 Compressed Interaction Network

为了实现自动学习显式的高阶特征交互，同时使得交互发生在向量级上，文中首先提出了一种新的名为**压缩交互网络（Compressed Interaction Network，简称CIN）的神经模型**。在CIN中，隐向量是一个单元对象，因此我们将输入的原特征和神经网络中的隐层都分别组织成一个矩阵，记为X0和Xk。CIN中每一层的神经元都是根据前一层的隐层以及原特征向量推算而来，其计算公式如下：

$$
\mathbf{X}_{h, *}^{k}=\sum_{i=1}^{H_{k-1}} \sum_{j=1}^{m} \mathbf{W}_{i j}^{k, h}\left(\mathbf{X}_{i, *}^{k-1} \circ \mathbf{X}_{j, *}^{0}\right)
$$

其中点乘的部分计算如下：

$$
<a_{1}, a_{2}, a_{3}>\cdot<b_{1}, b_{2}, b_{3}>=<a_{1} b_{1}, a_{2} b_{2}, a_{3} b_{3}>
$$

