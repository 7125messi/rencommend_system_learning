# 1、FM背景

在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与Tree系列。今天，我们就来讲讲FM算法。

<a name="Ijkf9"></a>
# 2、one-hot编码带来的问题

FM(Factorization Machine)主要是为了解决数据稀疏的情况下，特征怎样组合的问题。已一个广告分类的问题为例，根据用户与广告位的一些特征，来预测用户是否会点击广告。数据如下：(本例来自美团技术团队分享的paper)

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853860876-47ac2234-6a80-406f-a3aa-058235e7f89c.webp#align=left&display=inline&height=124&originHeight=124&originWidth=275&size=0&status=done&width=275)

clicked是分类值，表明用户有没有点击该广告。1表示点击，0表示未点击。而country,day,ad_type则是对应的特征。对于这种categorical特征，一般都是进行one-hot编码处理。<br />将上面的数据进行one-hot编码以后，就变成了下面这样 ：

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853860911-4a678f76-f7a2-4a0b-b7c6-5f1cfc6c383c.webp#align=left&display=inline&height=127&originHeight=127&originWidth=812&size=0&status=done&width=812)

因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。

one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。

<a name="qJkZa"></a>
# 3、对特征进行组合

普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。但实际上，大量的特征之间是有关联的。最简单的以电商为例，一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，女性这个特征与化妆品类服装类商品有很大的关联性，男性这个特征与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然是很有意义的。

一般的线性模型为：

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853860959-8ee06cb8-1c75-48c6-a7b5-3581a646bcc3.webp#align=left&display=inline&height=82&originHeight=82&originWidth=203&size=0&status=done&width=203)

从上面的式子很容易看出，一般的线性模型压根没有考虑特征间的关联。为了表述特征间的相关性，我们采用多项式模型。在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。具体的模型表达式如下：<br />![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861102-bd449a71-a9c4-42f8-a932-72a6d2e8eb5d.webp#align=left&display=inline&height=98&originHeight=98&originWidth=356&size=0&status=done&width=356)<br />上式中，n表示样本的特征数量,xi表示第i个特征。<br />与线性模型相比，FM的模型就多了后面特征组合的部分。

<a name="pTsgj"></a>
# 4、FM求解

从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。

为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861087-648a12ff-ecfd-4108-a0bb-cf8bf3dbe24f.webp#align=left&display=inline&height=146&originHeight=146&originWidth=403&size=0&status=done&width=403)

那么ωij组成的矩阵可以表示为:

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861215-c53d606c-aa19-4c32-9cd1-2b9955a5ab1e.webp#align=left&display=inline&height=106&originHeight=106&originWidth=337&size=0&status=done&width=337)

那么，如何求解vi和vj呢？主要采用了公式：

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861196-659fa1f4-22c6-4c94-9a69-c221a1bcd7fd.webp#align=left&display=inline&height=27&originHeight=27&originWidth=240&size=0&status=done&width=240)

具体过程如下：<br />![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861384-8d3795ba-66a7-4448-8c30-ab44789fae80.webp#align=left&display=inline&height=311&originHeight=311&originWidth=413&size=0&status=done&width=413)

上面的式子中有同学曾经问我第一步是怎么推导的，其实也不难，看下面的手写过程(大伙可不要嫌弃字丑哟)<br />![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861346-f3c4f0e5-48de-4a4e-8e7a-64e156c4980f.webp#align=left&display=inline&height=817&originHeight=817&originWidth=1000&size=0&status=done&width=1000)

经过这样的分解之后，我们就可以通过随机梯度下降SGD进行求解：

![](https://cdn.nlark.com/yuque/0/2019/webp/200056/1565853861633-501f7c5b-da06-442a-a2af-6dbc8d855108.webp#align=left&display=inline&height=99&originHeight=99&originWidth=389&size=0&status=done&width=389)