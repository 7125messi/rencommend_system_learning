排序推荐算法大体上可以分为三类:
- 第一类排序算法类别是**点对方法(Pointwise Approach)**，这类算法将排序问题被**转化为分类、回归之类的问题**，并使用现有分类、回归等方法进行实现。
- 第二类排序算法是**成对方法(Pairwise Approach)**，在序列方法中，排序被转化为**对序列分类或对序列回归**。所谓的pair就是成对的排序，比如(a,b)一组表明a比b排的靠前。
- 第三类排序算法是**列表方法(Listwise Approach)，它采用更加直接的方法对排序问题进行了处理**。它在**学习和预测过程中都将排序列表作为一个样本。排序的组结构被保持。**

之前我们介绍的算法大都是**Pointwise的方法**，今天我们来介绍**一种Pairwise的方法**：**贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)**

# 1、BPR算法简介

## 1.1 基本思路

**在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组<u,i,j>，它表示对用户u来说，i的排序要比j靠前。如果对于用户u来说我们有m组这样的反馈，那么我们就可以得到m组用户u对应的训练样本。**

这里，我们做出两个假设：
- 每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和其他用户无关。
- 同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和其他的商品无关。

为了便于表述，我们用>u符号表示用户u的偏好，上面的<u,i,j>可以表示为：i >u j。
在BPR中，我们也用到了类似矩阵分解的思想，对于用户集U和物品集I对应的U*I的预测排序矩阵，我们期望得到两个分解后的用户矩阵W(|U|×k)和物品矩阵H(|I|×k)，满足：

$$
\overline{X}=W H^{T}
$$

那么对于任意一个用户u，对应的任意一个物品i，我们预测得出的用户对该物品的偏好计算如下：

$$
\overline{x}_{u i}=w_{u} \bullet h_{i}=\sum_{f=1}^{k} w_{u f} h_{i f}
$$

而模型的最终目标是寻找合适的矩阵W和H，让X-(公式打不出来，这里代表的是X上面有一个横线，即W和H矩阵相乘后的结果)和X(实际的评分矩阵)最相似。看到这里，也许你会说，BPR和矩阵分解没有什区别呀？是的，到目前为止的基本思想是一致的，但是具体的算法运算思路，确实千差万别的，我们慢慢道来。

## 1.2 算法运算思路

BPR 基于最大后验估计P(W,H|>u)来求解模型参数W,H,这里我们用θ来表示参数W和H, >u代表用户u对应的所有商品的全序关系,则优化目标是P(θ|>u)。根据贝叶斯公式，我们有：

$$
P\left(\theta |>_{u}\right)=\frac{P\left(>_{u} | \theta\right) P(\theta)}{P\left(>_{u}\right)}
$$

由于我们求解假设了用户的排序和其他用户无关，那么对于任意一个用户u来说，P(>u)对所有的物品一样，所以有：

$$
P\left(\theta |>_{u}\right) \propto P\left(>_{u} | \theta\right) P(\theta)
$$

这个优化目标转化为两部分。第一部分和样本数据集D有关，第二部分和样本数据集D无关。

### 第一部分

对于第一部分，由于我们假设每个用户之间的偏好行为相互独立，同一用户对不同物品的偏序相互独立，所以有：

$$
\prod_{u \in U} P\left(>_{u} | \theta\right)=\prod_{(u, i, j) \in(U \times I \times I)} P\left(i>_{u} j | \theta\right)^{\delta((u, i, j) \in D)}\left(1-P\left(i>_{u} j | \theta\right)\right)^{\delta((u, j, i) \notin D)}
$$

上面的式子类似于极大似然估计，若用户u相比于j来说更偏向i，那么我们就希望P(i >u j|θ)出现的概率越大越好。

上面的式子可以进一步改写成：

$$
\prod_{u \in U} P\left(>_{u} | \theta\right)=\prod_{(u, i, j) \in D} P\left(i>_{u} j | \theta\right)
$$

而对于P(i >u j|θ)这个概率，我们可以使用下面这个式子来代替:

$$
P\left(i>_{u} j | \theta\right)=\sigma\left(\overline{x}_{u i j}(\theta)\right)
$$

其中，σ(x)是sigmoid函数，σ里面的项我们可以理解为用户u对i和j偏好程度的差异，我们当然希望i和j的差异越大越好，这种差异如何体现，最简单的就是差值：

$$
\overline{x}_{u i j}(\theta)=\overline{x}_{u i}(\theta)-\overline{x}_{u j}(\theta)
$$

省略θ我们可以将式子简略的写为：

$$
\overline{x}_{u i j}=\overline{x}_{u i}-\overline{x}_{u j}
$$

因此优化目标的第一项可以写作：

$$
\prod_{u \in U} P\left(>_{u} | \theta\right)=\prod_{(u, i, j) \in D} \sigma\left(\overline{x}_{u i}-\overline{x}_{u j}\right)
$$

哇，是不是很简单的思想，对于训练数据中的<u,i,j>，用户更偏好于i，那么我们当然希望在X-矩阵中ui对应的值比uj对应的值大，而且差距越大越好！

### 第二部分

通过贝叶斯角度解释正则化的文章：https://www.jianshu.com/p/4d562f2c06b8

当θ的先验分布是正态分布时，其实就是给损失函数加入了正则项，因此我们可以假定θ的先验分布是正态分布：

$$
P(\theta) \sim N\left(0, \lambda_{\theta} I\right)
$$

$$
\ln P(\theta)=\lambda\|\theta\|^{2}
$$

因此，最终的最大对数后验估计函数可以写作：

$$
\ln P\left(\theta |>_{u}\right) \propto \ln P\left(>_{u} | \theta\right) P(\theta)=\ln \prod_{(x, i, j) \in D} \sigma\left(\overline{x}_{u i}-\overline{x}_{u j}\right)+\ln P(\theta)=\sum_{(u, i, j) \in D} \ln \sigma\left(\overline{x}_{u i}-\overline{x}_{u j}\right)+\lambda\|\theta\|^{2}
$$

剩下的我们就可以通过梯度上升法(因为是要让上式最大化)来求解了。我们这里就略过了，BPR的思想已经很明白了吧，哈哈！让我们来看一看如何实现吧。

# 2 总结

- 1.BPR是基于矩阵分解的一种排序算法，它不是做全局的评分优化，而是针对每一个用户自己的商品喜好分贝做排序优化。
- 2.它是一种pairwise的排序算法，对于每一个三元组<u,i,j>，模型希望能够使用户u对物品i和j的差异更明显。
- 3.同时，引入了贝叶斯先验，假设参数服从正态分布，在转换后变为了L2正则，减小了模型的过拟合。

# 3 实现

所用到的数据集是movieslen 100k的数据集，下载地址为：http://grouplens.org/datasets/movielens/

```python
PS D:\Users\ZHAOYADONG706\project\rencommend_system_learning\rec_deeplearning\08-推荐系统贝叶斯个性化排序(BPR)算法原理及实战\code> python .\BPR.py
WARNING: Logging before flag parsing goes to stderr.
W0822 13:03:43.292960 43992 deprecation.py:323] From D:\anaconda\lib\site-packages\tensorflow\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
max_u_id: 943
max_i_idL 1682
2019-08-22 13:03:43.590816: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
W0822 13:03:43.707617 43992 deprecation.py:323] From .\BPR.py:92: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
epoch: 1
bpr_loss: 0.7237432103177075
_train_op
test_loss:  0.774585 test_auc:  0.4977460685218803

epoch: 2
bpr_loss: 0.7231077747622545
_train_op
test_loss:  0.77251863 test_auc:  0.49768003366063157

epoch: 3
bpr_loss: 0.7225093650898949
_train_op
test_loss:  0.7705373 test_auc:  0.49767488708468016

Variable:  user_emb_w:0
Shape:  (944, 20)
[[-0.10673332 -0.02308077 -0.05883194 ...  0.07538014 -0.03091449
  -0.0364668 ]
 [ 0.0109603  -0.02310606  0.03185152 ...  0.0636925  -0.07723708
   0.1739866 ]
 [-0.0552367  -0.02152224  0.2996567  ... -0.17957978 -0.04744763
   0.09724688]
 ...
 [ 0.00909243 -0.19014424  0.14398812 ... -0.09115276  0.0281064
  -0.02450201]
 [-0.02671943  0.15923533  0.10841027 ... -0.04724653 -0.0492784
   0.24586599]
 [-0.04278421 -0.0744108   0.0083751  ...  0.03398865  0.03205736
   0.05208488]]
Variable:  item_emb_w:0
Shape:  (1683, 20)
[[-0.0167312   0.0244645  -0.03368451 ... -0.13786235 -0.00422339
   0.02399852]
 [ 0.04490069 -0.12523578  0.03870344 ... -0.10028447 -0.07830234
   0.09604887]
 [ 0.08088767  0.11019749  0.16708922 ...  0.00489259  0.06374002
  -0.20674586]
 ...
 [-0.01013358  0.08492059 -0.0401105  ... -0.03850356 -0.16011885
  -0.07356734]
 [ 0.0520065  -0.17252012  0.15385886 ...  0.20365466 -0.03629278
   0.07620674]
 [-0.07605884 -0.08543079  0.02010741 ... -0.01119042  0.00970036
   0.11118511]]
[[-0.03459097 -0.0130521   0.00840135 ...  0.04847819 -0.06933779
  -0.01639593]]
以下是给用户0的推荐：
76 0.11731892
563 0.10473378
1076 0.11368022
1422 0.10423229
1532 0.10481111
```